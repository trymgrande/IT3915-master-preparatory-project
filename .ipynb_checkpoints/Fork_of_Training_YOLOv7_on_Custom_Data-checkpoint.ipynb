{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "  - test different:\n",
    "    - augmentations\n",
    "    - models\n",
    "        - v3/v7\n",
    "        - tiny/standard\n",
    "    - dataset sizes\n",
    "    - other parameters\n",
    "- code cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember:\n",
    "Restart kernel before running to reset current working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GD9gUQpaBxNa"
   },
   "source": [
    "# How to Train YOLOv7 on a Custom Dataset\n",
    "\n",
    "This tutorial is based on the [YOLOv7 repository](https://github.com/WongKinYiu/yolov7) by WongKinYiu. This notebook shows training on **your own custom objects**. Many thanks to WongKinYiu and AlexeyAB for putting this repository together.\n",
    "\n",
    "\n",
    "### **Accompanying Blog Post**\n",
    "\n",
    "We recommend that you follow along in this notebook while reading the blog post on [how to train YOLOv7](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/), concurrently.\n",
    "\n",
    "### **Steps Covered in this Tutorial**\n",
    "\n",
    "To train our detector we take the following steps:\n",
    "\n",
    "* Install YOLOv7 dependencies\n",
    "* Load custom dataset from Roboflow in YOLOv7 format\n",
    "* Run YOLOv7 training\n",
    "* Evaluate YOLOv7 performance\n",
    "* Run YOLOv7 inference on test images\n",
    "* OPTIONAL: Deployment\n",
    "* OPTIONAL: Active Learning\n",
    "\n",
    "\n",
    "### Preparing a Custom Dataset\n",
    "\n",
    "In this tutorial, we will utilize an open source computer vision dataset from one of the 90,000+ available on [Roboflow Universe](https://universe.roboflow.com).\n",
    "\n",
    "If you already have your own images (and, optionally, annotations), you can convert your dataset using [Roboflow](https://roboflow.com), a set of tools developers use to build better computer vision models quickly and accurately. 100k+ developers use roboflow for (automatic) annotation, converting dataset formats (like to YOLOv7), training, deploying, and improving their datasets/models.\n",
    "\n",
    "Follow [the getting started guide here](https://docs.roboflow.com/quick-start) to create and prepare your own custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "#Install Dependencies\n",
    "\n",
    "_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/trymg/IT3915-master-preparatory-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: roboflow in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.2.17)\n",
      "Requirement already satisfied: albumentations in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: opencv-python in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.6.0.66)\n",
      "Requirement already satisfied: six in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: cycler==0.10.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: urllib3==1.26.6 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (1.26.6)\n",
      "Requirement already satisfied: certifi==2021.5.30 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: requests-toolbelt in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: wget in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (3.2)\n",
      "Requirement already satisfied: chardet==4.0.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: matplotlib in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (3.3.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (8.4.0)\n",
      "Requirement already satisfied: glob2 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (0.7)\n",
      "Requirement already satisfied: python-dotenv in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: idna==2.10 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: python-dateutil in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: requests in /cluster/home/trymg/.local/lib/python3.6/site-packages (from roboflow->-r requirements.txt (line 1)) (2.27.1)\n",
      "Requirement already satisfied: scipy in /cluster/home/trymg/.local/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 2)) (0.0.4)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 2)) (0.17.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from qudida>=0.0.4->albumentations->-r requirements.txt (line 2)) (0.24.2)\n",
      "Requirement already satisfied: typing-extensions in /cluster/home/trymg/.local/lib/python3.6/site-packages (from qudida>=0.0.4->albumentations->-r requirements.txt (line 2)) (4.1.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 2)) (2020.9.3)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 2)) (2.15.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 2)) (2.5.1)\n",
      "Requirement already satisfied: importlib-resources in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tqdm>=4.41.0->roboflow->-r requirements.txt (line 1)) (5.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from requests->roboflow->-r requirements.txt (line 1)) (2.0.12)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 2)) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from importlib-resources->tqdm>=4.41.0->roboflow->-r requirements.txt (line 1)) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nD-uPyQ_2jiN",
    "outputId": "fbbc3b6b-3dce-471e-f9d2-a4703201ff52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov7' already exists and is not an empty directory.\n",
      "/cluster/home/trymg/IT3915-master-preparatory-project/yolov7\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (4.6.0.66)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (8.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (2.27.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 10)) (1.5.4)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 11)) (1.10.1)\n",
      "Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (0.11.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 13)) (4.64.1)\n",
      "Requirement already satisfied: protobuf<4.21.3 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 14)) (3.19.6)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 17)) (2.10.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 21)) (1.1.5)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 22)) (0.11.2)\n",
      "Requirement already satisfied: ipython in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 34)) (7.16.3)\n",
      "Requirement already satisfied: psutil in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 35)) (5.9.3)\n",
      "Requirement already satisfied: thop in /cluster/home/trymg/.local/lib/python3.6/site-packages (from -r requirements.txt (line 36)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.0.12)\n",
      "Requirement already satisfied: dataclasses in /cluster/home/trymg/.local/lib/python3.6/site-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /cluster/home/trymg/.local/lib/python3.6/site-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (4.1.1)\n",
      "Requirement already satisfied: importlib-resources in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tqdm>=4.41.0->-r requirements.txt (line 13)) (5.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.3.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.48.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.13.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.0.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (59.6.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3.6/site-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2017.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from ipython->-r requirements.txt (line 34)) (4.3.3)\n",
      "Requirement already satisfied: jedi<=0.17.2,>=0.10 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from ipython->-r requirements.txt (line 34)) (0.17.2)\n",
      "Requirement already satisfied: decorator in /cluster/home/trymg/.local/lib/python3.6/site-packages (from ipython->-r requirements.txt (line 34)) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /cluster/home/trymg/.local/lib/python3.6/site-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from ipython->-r requirements.txt (line 34)) (3.0.31)\n",
      "Requirement already satisfied: pexpect in /cluster/home/trymg/.local/lib/python3.6/site-packages (from ipython->-r requirements.txt (line 34)) (4.8.0)\n",
      "Requirement already satisfied: pygments in /cluster/home/trymg/.local/lib/python3.6/site-packages (from ipython->-r requirements.txt (line 34)) (2.13.0)\n",
      "Requirement already satisfied: backcall in /cluster/home/trymg/.local/lib/python3.6/site-packages (from ipython->-r requirements.txt (line 34)) (0.2.0)\n",
      "Requirement already satisfied: six in /cluster/home/trymg/.local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.3.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from jedi<=0.17.2,>=0.10->ipython->-r requirements.txt (line 34)) (0.7.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: importlib-metadata>=4.4 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.8.3)\n",
      "Requirement already satisfied: wcwidth in /cluster/home/trymg/.local/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 34)) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /cluster/home/trymg/.local/lib/python3.6/site-packages (from traitlets>=4.2->ipython->-r requirements.txt (line 34)) (0.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from importlib-resources->tqdm>=4.41.0->-r requirements.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from pexpect->ipython->-r requirements.txt (line 34)) (0.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /cluster/home/trymg/.local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Download YOLOv7 repository and install requirements\n",
    "!git clone https://github.com/WongKinYiu/yolov7\n",
    "%cd yolov7\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_NAME = \"Merged-sheep-dataset\"\n",
    "PROJECT_VERSION = 3\n",
    "PROJECT_DIRECTORY = f\"{PROJECT_NAME}-{PROJECT_VERSION}\"\n",
    "\n",
    "AUGMENTATION = False\n",
    "AUGMENTATIONS = float('inf')\n",
    "# AUGMENTATIONS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtJ24mPlyF-S"
   },
   "source": [
    "# Download Correctly Formatted Custom Data\n",
    "\n",
    "Next, we'll download our dataset in the right format. Use the `YOLOv7 PyTorch` export. Note that this model requires YOLO TXT annotations, a custom YAML file, and organized directories. The roboflow export writes this for us and saves it in the correct spot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "# print(rf.workspace().projects()) # for debugging\n",
    "\n",
    "# custom code snippet generated from roboflow dataset using the export function\n",
    "\n",
    "# # old dataset with 4k images:\n",
    "# rf = Roboflow(api_key=\"Sf4Q132h8vYyzxbVfF7t\")\n",
    "# project = rf.workspace(\"it3915masterpreparatoryproject\").project(\"sheep-detection-2\")\n",
    "# dataset = project.version(3).download(\"yolov7\")\n",
    "\n",
    "# new merged dataset with 6k images:\n",
    "#rf = Roboflow(api_key=\"Sf4Q132h8vYyzxbVfF7t\")\n",
    "#project = rf.workspace(\"it3915masterpreparatoryproject\").project(\"merged-sheep-dataset\")\n",
    "#dataset = project.version(PROJECT_VERSION).download(\"yolov7\")\n",
    "\n",
    "\n",
    "# os.rename(f\"./yolov7/{PROJECT_DIRECTORY}\", f\"./yolov7/{PROJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom image augmentation using Albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation plot utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "\n",
    "def plot_examples(images, bboxes=None):\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    columns = 4\n",
    "    rows = 5\n",
    "\n",
    "    for i in range(1, len(images)):\n",
    "        if bboxes is not None:\n",
    "            img = visualize_bbox(images[i - 1], bboxes[i - 1], class_name=\"sheep\")\n",
    "        else:\n",
    "            img = images[i-1]\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# From https://albumentations.ai/docs/examples/example_bboxes/\n",
    "def visualize_bbox(img, bbox, class_name, color=(255, 0, 0), thickness=5):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "    # cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "    cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, thickness)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/examples/example_bboxes2/\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from: https://www.kaggle.com/code/reighns/augmentations-data-cleaning-and-bounding-boxes/notebook\n",
    "\n",
    "def draw_rect_with_labels(img, bboxes,class_id, class_dict={1: 'sheep'}, color=None):\n",
    "    img = img.copy()\n",
    "    bboxes = bboxes[:, :4]\n",
    "    bboxes = bboxes.reshape(-1, 4)\n",
    "    for bbox, label in zip(bboxes, class_id):\n",
    "        pt1, pt2 = (bbox[0], bbox[1]), (bbox[2], bbox[3])\n",
    "        pt1 = int(pt1[0]), int(pt1[1])\n",
    "        pt2 = int(pt2[0]), int(pt2[1])\n",
    "        class_name = class_dict[label]\n",
    "        ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1) \n",
    "        img = cv2.rectangle(img.copy(), pt1, pt2, color, int(max(img.shape[:2]) / 200))\n",
    "        img = cv2.putText(img.copy(), class_name, (int(bbox[0]), int(bbox[1]) - int(0.3 * text_height)), cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color = (255,255,255), lineType=cv2.LINE_AA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import inf\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, os.path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUGMENTATION: \n",
    "    CLASS_LABEL = 'sheep'\n",
    "    CLASS = 0\n",
    "\n",
    "    # load train images\n",
    "    images = []\n",
    "    images_load_path = f\"./{PROJECT_DIRECTORY}/train/images/\"\n",
    "    valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "\n",
    "    LIMIT = AUGMENTATIONS\n",
    "    i = 0\n",
    "    for image_file_name in os.listdir(images_load_path):\n",
    "        if i == LIMIT:\n",
    "            break\n",
    "        ext = os.path.splitext(image_file_name)[1]\n",
    "        if ext.lower() not in valid_images:\n",
    "            continue\n",
    "        images.append(cv2.imread(os.path.join(images_load_path, image_file_name)))\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    # load train labels\n",
    "    labels = [] # one label per image, multiple bboxes per image\n",
    "    labels_load_path = f\"./{PROJECT_DIRECTORY}/train/labels/\"\n",
    "    i = 0\n",
    "    for label_file_name in os.listdir(labels_load_path):\n",
    "        if i == LIMIT:\n",
    "            break\n",
    "        label = open(os.path.join(labels_load_path, label_file_name), \"r\")\n",
    "        bboxes = []\n",
    "        for bbox in label:\n",
    "            bbox = np.array(bbox.split(' ')[1:]).astype(np.float32)\n",
    "            bboxes.append(bbox)\n",
    "        labels.append(bboxes)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "\n",
    "    # augment images using transformation\n",
    "\n",
    "    augmented_labels = []\n",
    "    augmented_images_nd = []\n",
    "    for image, label in zip(images, labels):\n",
    "        label = np.array(label)\n",
    "\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.RandomCrop(width=640, height=640),\n",
    "                A.Rotate(border_mode=cv2.BORDER_CONSTANT),\n",
    "                A.HorizontalFlip(),\n",
    "                A.VerticalFlip(),\n",
    "                A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.9),\n",
    "                A.OneOf([\n",
    "                    A.Blur(blur_limit=3, p=0.5),\n",
    "                    A.ColorJitter(p=0.5),\n",
    "                ], p=1.0),\n",
    "                A.RandomBrightnessContrast()\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])\n",
    "        )\n",
    "        \n",
    "        augmentation = transform(image=image, bboxes=label, class_labels=[CLASS_LABEL]*len(label))\n",
    "        augmented_img = augmentation[\"image\"]\n",
    "        augmented_label = augmentation[\"bboxes\"]\n",
    "        augmented_labels.append(augmented_label)\n",
    "        # augmented_category_ids = [int(CLASS_LABEL)]*len(label)\n",
    "        augmented_images_nd.append(augmented_img)\n",
    "        \n",
    "        # draw_image = draw_rect_with_labels(augmented_img, label, [1]*len(label))\n",
    "\n",
    "        # visualize(\n",
    "        #     augmented_img,\n",
    "        #     augmented_label,\n",
    "        #     augmented_category_ids,\n",
    "        #     {1: 'sheep', 2: 'sheep', 3: 'sheep', 0: 'sheep'}\n",
    "        # )\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.imshow(augmented_img)\n",
    "\n",
    "\n",
    "        # augment_and_show(transform, image)\n",
    "\n",
    "\n",
    "    # plot_examples(augmented_images_nd) # comment out to save runtime\n",
    "\n",
    "\n",
    "    augmented_images = [Image.fromarray(augmented_image_nd) for augmented_image_nd in augmented_images_nd]\n",
    "\n",
    "    # create image save dir\n",
    "    img_save_dir = f\"./{PROJECT_DIRECTORY}/train/images\"\n",
    "\n",
    "\n",
    "    # save augmented images\n",
    "    for i in range(len(augmented_images)):\n",
    "        image_file_name = f\"{i}.png\"\n",
    "        image_dir = os.path.join(img_save_dir, image_file_name)\n",
    "        augmented_images[i].save(image_dir)\n",
    "        print(f\"saved augmented image: {image_dir}\")\n",
    "\n",
    "\n",
    "    # create labels save dir\n",
    "    labels_save_dir = f\"./{PROJECT_DIRECTORY}/train/labels\"\n",
    "\n",
    "    # write augmented labels\n",
    "    for i in range(len(augmented_labels)):\n",
    "        label_file_name = f\"{i}.txt\"\n",
    "        label_dir = os.path.join(labels_save_dir, label_file_name)\n",
    "        with open(label_dir, 'w') as f:\n",
    "            for bbox in augmented_labels[i]:\n",
    "                f.write(f\"{CLASS} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\")\n",
    "        print(f\"saved augmented label: {label_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHfT9gEiBsBd"
   },
   "source": [
    "# Begin Custom Training\n",
    "\n",
    "We're ready to start custom training.\n",
    "\n",
    "NOTE: We will only modify one of the YOLOv7 training defaults in our example: `epochs`. We will adjust from 300 to 100 epochs in our example for speed. If you'd like to change other settings, see details in [our accompanying blog post](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUbmy674bhpD",
    "outputId": "081fb899-7c47-4af4-ff1b-565b7d33b4f7"
   },
   "outputs": [],
   "source": [
    "# download COCO starting checkpoint (weights)\n",
    "# !wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image results from previous runs are deleted\n",
    "!rm runs/train/* -rf \n",
    "!rm runs/detect/* -rf\n",
    "# may need to remove labels.cache and/or images.cache beforehand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete labels.cache if labels have changed\n",
    "!rm Merged-sheep-dataset-3/train/labels.cache -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.device_count()\n",
    "#torch.cuda.current_device()\n",
    "#torch.cuda.device(0)\n",
    "#torch.cuda.get_device_name(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install wandb -qU\n",
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iqOPKjr22mL",
    "outputId": "7ae0f2c4-d080-4bee-8f8d-e52d9f789c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOR 🚀 v0.1-115-g072f76c torch 1.10.1+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.9375MB)\n",
      "\n",
      "Namespace(adam=False, artifact_alias='latest', batch_size=500, bbox_interval=-1, bucket='', cache_images=False, cfg='', data='Merged-sheep-dataset-3/data.yaml', device='0', entity=None, epochs=20, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.p5.yaml', image_weights=False, img_size=[140, 140], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/exp10', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=500, upload_dataset=False, v5_metric=False, weights='yolov7-tiny.pt', workers=8, world_size=1)\n",
      "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
      "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  2                -1  1      2112  models.common.Conv                      [64, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  3                -2  1      2112  models.common.Conv                      [64, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  4                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  5                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  6  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      "  7                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      "  8                -1  1         0  models.common.MP                        []                            \n",
      "  9                -1  1      4224  models.common.Conv                      [64, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 10                -2  1      4224  models.common.Conv                      [64, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 11                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 12                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 13  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 15                -1  1         0  models.common.MP                        []                            \n",
      " 16                -1  1     16640  models.common.Conv                      [128, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 17                -2  1     16640  models.common.Conv                      [128, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 20  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 22                -1  1         0  models.common.MP                        []                            \n",
      " 23                -1  1     66048  models.common.Conv                      [256, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 24                -2  1     66048  models.common.Conv                      [256, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 25                -1  1    590336  models.common.Conv                      [256, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 26                -1  1    590336  models.common.Conv                      [256, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 27  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 28                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 29                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 30                -2  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 31                -1  1         0  models.common.SP                        [5]                           \n",
      " 32                -2  1         0  models.common.SP                        [9]                           \n",
      " 33                -3  1         0  models.common.SP                        [13]                          \n",
      " 34  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 35                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 36          [-1, -7]  1         0  models.common.Concat                    [1]                           \n",
      " 37                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 38                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 39                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 40                21  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 41          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 42                -1  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 43                -2  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 44                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 45                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 46  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 47                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 48                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 49                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 50                14  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 51          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 52                -1  1      4160  models.common.Conv                      [128, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 53                -2  1      4160  models.common.Conv                      [128, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 54                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 55                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 57                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 58                -1  1     73984  models.common.Conv                      [64, 128, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 59          [-1, 47]  1         0  models.common.Concat                    [1]                           \n",
      " 60                -1  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 61                -2  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 62                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 63                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 64  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 65                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 66                -1  1    295424  models.common.Conv                      [128, 256, 3, 2, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 67          [-1, 37]  1         0  models.common.Concat                    [1]                           \n",
      " 68                -1  1     65792  models.common.Conv                      [512, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 69                -2  1     65792  models.common.Conv                      [512, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 70                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 71                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 72  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 73                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 74                57  1     73984  models.common.Conv                      [64, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 75                65  1    295424  models.common.Conv                      [128, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 76                73  1   1180672  models.common.Conv                      [256, 512, 3, 1, None, 1, LeakyReLU(negative_slope=0.1)]\n",
      " 77      [74, 75, 76]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 255 layers, 6014038 parameters, 6014038 gradients, 13.2 GFLOPS\n",
      "\n",
      "Transferred 332/338 items from yolov7-tiny.pt\n",
      "Scaled weight_decay = 0.00390625\n",
      "Optimizer groups: 58 .bias, 58 conv.weight, 55 other\n",
      "WARNING: --img-size 140 must be multiple of max stride 32, updating to 160\n",
      "WARNING: --img-size 140 must be multiple of max stride 32, updating to 160\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'Merged-sheep-dataset-3/train/labels.cache' images and labels...\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'Merged-sheep-dataset-3/valid/labels.cache' images and labels... 1\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 1.06, Best Possible Recall (BPR) = 0.7681. Attempting to improve anchors, please wait...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 20202 of 105851 labels are < 3 pixels in size.\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 103978 points...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9988 best possible recall, 7.91 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=160, metric_all=0.498/0.811-mean/best, past_thr=0.541-mean: 2,3,  4,8,  3,11,  6,6,  4,14,  8,8,  6,11,  12,14,  8,20\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8306: 100%|█| 1\u001b[0m\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9993 best possible recall, 7.91 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=160, metric_all=0.507/0.825-mean/best, past_thr=0.552-mean: 2,3,  5,5,  3,11,  4,8,  7,7,  4,12,  6,10,  7,16,  11,15\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
      "\n",
      "Image sizes 160 train, 160 test\n",
      "Using 8 dataloader workers\n",
      "Logging results to runs/train/exp10\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 616, in <module>\n",
      "    train(hyp, opt, device, tb_writer)\n",
      "  File \"train.py\", line 361, in train\n",
      "    pred = model(imgs)  # forward\n",
      "  File \"/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/cluster/home/trymg/IT3915-master-preparatory-project/yolov7/models/yolo.py\", line 599, in forward\n",
      "    return self.forward_once(x, profile)  # single-scale inference, train\n",
      "  File \"/cluster/home/trymg/IT3915-master-preparatory-project/yolov7/models/yolo.py\", line 625, in forward_once\n",
      "    x = m(x)  # run\n",
      "  File \"/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/cluster/home/trymg/IT3915-master-preparatory-project/yolov7/models/common.py\", line 108, in forward\n",
      "    return self.act(self.bn(self.conv(x)))\n",
      "  File \"/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.27 GiB (GPU 0; 15.90 GiB total capacity; 1.50 GiB already allocated; 12.56 GiB free; 2.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --batch-size 1 --epochs 20 --data Merged-sheep-dataset-3/data.yaml --weights 'yolov7-tiny.pt' --device 0 --img 640 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup augmentations after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo check if dir not empty\n",
    "\n",
    "if AUGMENTATION: \n",
    "\n",
    "    # remove augmented images\n",
    "    img_save_dir = f\"./{PROJECT_DIRECTORY}/train/images\"\n",
    "    for i in range(len(augmented_images)):\n",
    "        image_file_name = f\"{i}.png\"\n",
    "        image_dir = os.path.join(img_save_dir, image_file_name)\n",
    "        os.remove(os.path.join(img_save_dir, image_file_name))\n",
    "        print(f\"removed image: {image_dir}\")\n",
    "\n",
    "    # remove augmented labels\n",
    "    labels_save_dir = f\"./{PROJECT_DIRECTORY}/train/labels\"\n",
    "    for i in range(len(labels)):\n",
    "        label_file_name = f\"{i}.txt\"\n",
    "        label_dir = os.path.join(labels_save_dir, label_file_name)\n",
    "        os.remove(os.path.join(labels_save_dir, label_file_name))\n",
    "        print(f\"removed label: {label_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W0MpUaTCJro"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "We can evaluate the performance of our custom training using the provided evalution script.\n",
    "\n",
    "Note we can adjust the below custom arguments. For details, see [the arguments accepted by detect.py](https://github.com/WongKinYiu/yolov7/blob/main/detect.py#L154)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "N4cfnLtTCIce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.3, device='', exist_ok=False, img_size=640, iou_thres=0.45, name='exp', no_trace=False, nosave=False, project='runs/detect', save_conf=False, save_txt=False, source='Merged-sheep-dataset-3/train/images', update=False, view_img=False, weights=['runs/train/exp/weights/best.pt'])\n",
      "YOLOR 🚀 v0.1-115-g072f76c torch 1.10.1+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.9375MB)\n",
      "                                             CUDA:1 (Tesla P100-PCIE-16GB, 16280.9375MB)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"detect.py\", line 196, in <module>\n",
      "    detect()\n",
      "  File \"detect.py\", line 34, in detect\n",
      "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
      "  File \"/cluster/home/trymg/IT3915-master-preparatory-project/yolov7/models/experimental.py\", line 252, in attempt_load\n",
      "    ckpt = torch.load(w, map_location=map_location)  # load\n",
      "  File \"/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/serialization.py\", line 594, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/serialization.py\", line 230, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/cluster/home/trymg/.local/lib/python3.6/site-packages/torch/serialization.py\", line 211, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'runs/train/exp/weights/best.pt'\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "!python3 detect.py --weights runs/train/exp/weights/best.pt --conf 0.3 --source Merged-sheep-dataset-3/train/images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6AGhNOSSHY4_"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "i = 0\n",
    "limit = 10000 # max images to print\n",
    "for imageName in glob.glob('runs/detect/exp/*.jpg'): #assuming JPG\n",
    "    print(imageName)\n",
    "    if i < limit:\n",
    "      display(Image(filename=imageName))\n",
    "      print(\"\\n\")\n",
    "    i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMumI7a2JDAN"
   },
   "source": [
    "# Reparameterize for Inference\n",
    "\n",
    "https://github.com/WongKinYiu/yolov7/blob/main/tools/reparameterization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jn4kCtgKiGO"
   },
   "source": [
    "# OPTIONAL: Deployment\n",
    "\n",
    "To deploy, you'll need to export your weights and save them to use later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wWOok8abrCsL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: runs/detect/ (stored 0%)\n",
      "updating: runs/detect/exp/ (stored 0%)\n",
      "\tzip warning: name not matched: runs/train/exp/weights/best.pt\n",
      "\n",
      "zip error: Nothing to do! (try: zip -r export.zip . -i runs/train/exp/weights/best.pt)\n",
      "updating: runs/train/exp/hyp.yaml (deflated 44%)\n",
      "updating: runs/train/exp/opt.yaml (deflated 46%)\n",
      "updating: runs/train/exp/weights/ (stored 0%)\n",
      "  adding: runs/train/exp/events.out.tfevents.1668780424.idun-login1.2646814.0 (deflated 22%)\n"
     ]
    }
   ],
   "source": [
    "# # optional, zip to download weights and results locally\n",
    "\n",
    "!zip -r export.zip runs/detect\n",
    "!zip -r export.zip runs/train/exp/weights/best.pt\n",
    "!zip export.zip runs/train/exp/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f41PvE5gKhYw"
   },
   "source": [
    "# OPTIONAL: Active Learning Example\n",
    "\n",
    "Once our first training run is complete, we should use our model to help identify which images are most problematic in order to investigate, annotate, and improve our dataset (and, therefore, model).\n",
    "\n",
    "To do that, we can execute code that automatically uploads images back to our hosted dataset if the image is a specific class or below a given confidence threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mcINqQS7Kt3-"
   },
   "outputs": [],
   "source": [
    "# # setup access to your workspace\n",
    "# rf = Roboflow(api_key=\"YOUR_API_KEY\")                               # used above to load data\n",
    "# inference_project =  rf.workspace().project(\"YOUR_PROJECT_NAME\")    # used above to load data\n",
    "# model = inference_project.version(1).model\n",
    "\n",
    "# upload_project = rf.workspace().project(\"YOUR_PROJECT_NAME\")\n",
    "\n",
    "# print(\"inference reference point: \", inference_project)\n",
    "# print(\"upload destination: \", upload_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "cEl1NVE3LSD_"
   },
   "outputs": [],
   "source": [
    "# # example upload: if prediction is below a given confidence threshold, upload it \n",
    "\n",
    "# confidence_interval = [10,70]                                   # [lower_bound_percent, upper_bound_percent]\n",
    "\n",
    "# for prediction in predictions:                                  # predictions list to loop through\n",
    "#   if(prediction['confidence'] * 100 >= confidence_interval[0] and \n",
    "#           prediction['confidence'] * 100 <= confidence_interval[1]):\n",
    "        \n",
    "#           # upload on success!\n",
    "#           print(' >> image uploaded!')\n",
    "#           upload_project.upload(image, num_retry_uploads=3)     # upload image in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVpCFeU-K4gb"
   },
   "source": [
    "# Next steps\n",
    "\n",
    "Congratulations, you've trained a custom YOLOv7 model! Next, start thinking about deploying and [building an MLOps condaeline](https://docs.roboflow.com) so your model gets better the more data it sees in the wild."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5d025aa593f62eb40ad5cb5f266804b6a3d4969bfd7696c6f6ca5bb2179774a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
